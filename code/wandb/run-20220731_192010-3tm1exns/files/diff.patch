diff --git a/code/BioBertEmbeddings.py b/code/BioBertEmbeddings.py
index fc7359d..48d5463 100644
--- a/code/BioBertEmbeddings.py
+++ b/code/BioBertEmbeddings.py
@@ -1,6 +1,25 @@
 import nlu
+import pandas as pd
+import numpy as np
+from sklearn.manifold import TSNE
+import seaborn as sns
 
-pipe = nlu.load(biobert)
+pipe = nlu.load('biobert')
 
-pipe.predict("He is amazing")
+#Read the csv
+df = pd.read_csv('../data/ner-disease/DatasetTrain.csv')
+df['text'] = df['Word']
+# NLU to gives us one row per embedded word by specifying the output level
+predictions = pipe.predict(df[['text','Tag']], output_level='token')
+print(predictions)
+predictions.dropna(how='any', inplace=True)
 
+mat = np.matrix([x for x in predictions.biobert_embeddings])
+
+model = TSNE(n_components=2)
+low_dim_data = model.fit_transform(mat)
+print('Lower dim data has shape',low_dim_data.shape)
+
+tsne_df =  pd.DataFrame(low_dim_data, predictions.pos)
+ax = sns.scatterplot(data=tsne_df, x=0, y=1, hue=tsne_df.index)
+ax.set_title('T-SNE BIOBERT Embeddings, colored by Part of Speech Tag')
\ No newline at end of file
diff --git a/code/TrainNew.py b/code/TrainNew.py
index b319db5..8bf5f8d 100644
--- a/code/TrainNew.py
+++ b/code/TrainNew.py
@@ -55,7 +55,7 @@ class NeuralNetwork(object):
         X = pad_sequences(maxlen=self.max_len, sequences=X, padding="post", value=word2idx["PAD"])
         y = [[tag2idx[l_i] for l_i in l] for l in labels]
         y = pad_sequences(maxlen=self.max_len, sequences=y, padding="post", value=tag2idx["|O\n"])
-        self.X_tr, self.X_te, self.y_tr, self.y_te = train_test_split(X, y, test_size=0.1, shuffle=False)
+        self.X_tr, self.X_te, self.y_tr, self.y_te = train_test_split(X, y, test_size=0.2, shuffle=False)
         print("Completed till split")
     
     def LSTM_NN(self):
@@ -71,7 +71,7 @@ class NeuralNetwork(object):
         model = keras.Model(word_input, out)
         opt = keras.optimizers.Adam(learning_rate = 0.001)
         model.compile(optimizer=opt, loss="sparse_categorical_crossentropy", metrics=["accuracy"])
-        self.history = model.fit(self.X_tr, self.y_tr.reshape(*self.y_tr.shape, 1), batch_size=32, epochs=5, validation_split=0.2, verbose=1, callbacks=[WandbCallback()])
+        self.history = model.fit(self.X_tr, self.y_tr.reshape(*self.y_tr.shape, 1), batch_size=32, epochs=20, validation_split=0.2, verbose=1, callbacks=[WandbCallback()])
         name = '../models/' + 'ckpt' +str(time.time()) + '.h5'
         model.save(name)
         print("Model saved in model directory...")
diff --git a/code/__pycache__/TrainNew.cpython-310.pyc b/code/__pycache__/TrainNew.cpython-310.pyc
index 7a11c7a..bd878da 100644
Binary files a/code/__pycache__/TrainNew.cpython-310.pyc and b/code/__pycache__/TrainNew.cpython-310.pyc differ
diff --git a/code/wandb/debug-internal.log b/code/wandb/debug-internal.log
index f21f872..148cd26 120000
--- a/code/wandb/debug-internal.log
+++ b/code/wandb/debug-internal.log
@@ -1 +1 @@
-run-20220728_123320-1roh71fu/logs/debug-internal.log
\ No newline at end of file
+run-20220731_192010-3tm1exns/logs/debug-internal.log
\ No newline at end of file
diff --git a/code/wandb/debug.log b/code/wandb/debug.log
index bec43e1..5b759e4 120000
--- a/code/wandb/debug.log
+++ b/code/wandb/debug.log
@@ -1 +1 @@
-run-20220728_123320-1roh71fu/logs/debug.log
\ No newline at end of file
+run-20220731_192010-3tm1exns/logs/debug.log
\ No newline at end of file
diff --git a/code/wandb/latest-run b/code/wandb/latest-run
index ebee3e9..0e4a5c3 120000
--- a/code/wandb/latest-run
+++ b/code/wandb/latest-run
@@ -1 +1 @@
-run-20220728_123320-1roh71fu
\ No newline at end of file
+run-20220731_192010-3tm1exns
\ No newline at end of file
diff --git a/data/tag2idx.pkl b/data/tag2idx.pkl
index 78cbcbf..0d326ab 100644
Binary files a/data/tag2idx.pkl and b/data/tag2idx.pkl differ
diff --git a/data/word2idx.pkl b/data/word2idx.pkl
index 18a6389..63c0e53 100644
Binary files a/data/word2idx.pkl and b/data/word2idx.pkl differ
