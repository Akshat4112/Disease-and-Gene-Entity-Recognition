{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c2e79e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import operator\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from plot_keras_history import plot_history\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from keras_contrib.utils import save_load_utils\n",
    "\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "\n",
    "# from keras_contrib.layers import CRF\n",
    "from keras_contrib import losses\n",
    "from keras_contrib import metrics\n",
    "\n",
    "from Preprocess import Preprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from tensorflow_addons.layers import CRF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6c0b80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = '../data/ner-disease/train.iob'\n",
    "test = '../data/ner-disease/test.iob' \n",
    "dev = '../data/ner-disease/dev.iob'\n",
    "dev_predicted = '../data/ner-disease/dev-predicted.iob'\n",
    "\n",
    "\n",
    "preprocess = Preprocess()\n",
    "preprocess.text_to_data(filepath=train)\n",
    "X, y = preprocess.preprocess_data()\n",
    "preprocess.text_to_data(filepath=test)\n",
    "Xtest, y_true = preprocess.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a2b4a223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Sentence', 'Word', 'POS', 'Tag'])\n",
    "df['Word'] = X\n",
    "df['Tag'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5390c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df['Word'])):\n",
    "    item = df['Word'][i]\n",
    "    tag = nltk.pos_tag([item])\n",
    "    df['POS'][i] = tag[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089470e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentence'][0] = 'Sentence: '+ str(1)\n",
    "k = 2\n",
    "\n",
    "for i in range(len(df['Word'])):\n",
    "    if df['Word'][i] == '.':\n",
    "        df['Sentence'][i+1] = 'Sentence: ' + str(k)\n",
    "        k+=1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6e78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfnew['Sentence'] = dfnew['Sentence'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71169f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew = dfnew.fillna(method=\"ffill\")\n",
    "dfnew[\"Sentence\"] = dfnew[\"Sentence\"].apply(lambda s: s[9:])\n",
    "# dfnew[\"Sentence\"] = dfnew[\"Sentence\"].astype(\"int32\")\n",
    "dfnew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de95989",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.to_csv('../data/ner-disease/DatasetTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9226717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of sentences in the dataset: {:,}\".format(dfnew[\"Sentence\"].nunique()))\n",
    "print(\"Total words in the dataset: {:,}\".format(dfnew.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew[\"POS\"].value_counts().plot(kind=\"bar\", figsize=(10,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew[dfnew[\"Tag\"]!=\"O\"][\"Tag\"].value_counts().plot(kind=\"bar\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = dfnew.groupby(\"Sentence\")[\"Word\"].agg([\"count\"])\n",
    "word_counts = word_counts.rename(columns={\"count\": \"Word count\"})\n",
    "word_counts.hist(bins=50, figsize=(8,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd00dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE = word_counts.max()[0]\n",
    "print(\"Longest sentence in the corpus contains {} words.\".format(MAX_SENTENCE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b22ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_sentence_id = word_counts[word_counts[\"Word count\"]==MAX_SENTENCE].index[0]\n",
    "print(\"ID of the longest sentence is {}.\".format(longest_sentence_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0588bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_sentence = dfnew[dfnew[\"Sentence\"]==longest_sentence_id][\"Word\"].str.cat(sep=' ')\n",
    "print(\"The longest sentence in the corpus is:\\n\")\n",
    "print(longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd657f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(set(dfnew[\"Word\"].values))\n",
    "all_tags = list(set(dfnew[\"Tag\"].values))\n",
    "\n",
    "print(\"Number of unique words: {}\".format(dfnew[\"Word\"].nunique()))\n",
    "print(\"Number of unique tags : {}\".format(dfnew[\"Tag\"].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceebc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {word: idx + 2 for idx, word in enumerate(all_words)}\n",
    "\n",
    "word2index[\"--UNKNOWN_WORD--\"]=0\n",
    "\n",
    "word2index[\"--PADDING--\"]=1\n",
    "\n",
    "index2word = {idx: word for word, idx in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sorted(word2index.items(), key=operator.itemgetter(1))[:10]:\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = \"examinations\"\n",
    "\n",
    "test_word_idx = word2index[test_word]\n",
    "test_word_lookup = index2word[test_word_idx]\n",
    "\n",
    "print(\"The index of the word {} is {}.\".format(test_word, test_word_idx))\n",
    "print(\"The word with index {} is {}.\".format(test_word_idx, test_word_lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2index = {tag: idx + 1 for idx, tag in enumerate(all_tags)}\n",
    "tag2index[\"--PADDING--\"] = 0\n",
    "\n",
    "index2tag = {idx: word for word, idx in tag2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50afd9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tuples(data):\n",
    "    iterator = zip(data[\"Word\"].values.tolist(),\n",
    "                   data[\"POS\"].values.tolist(),\n",
    "                   data[\"Tag\"].values.tolist())\n",
    "    return [(word, pos, tag) for word, pos, tag in iterator]\n",
    "\n",
    "sentences = dfnew.groupby(\"Sentence\").apply(to_tuples).tolist()\n",
    "\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c117ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[word[0] for word in sentence] for sentence in sentences]\n",
    "y = [[word[2] for word in sentence] for sentence in sentences]\n",
    "print(\"X[0]:\", X[0])\n",
    "print(\"y[0]:\", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[word2index[word] for word in sentence] for sentence in X]\n",
    "y = [[tag2index[tag] for tag in sentence] for sentence in y]\n",
    "print(\"X[0]:\", X[0])\n",
    "print(\"y[0]:\", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sentence + [word2index[\"--PADDING--\"]] * (MAX_SENTENCE - len(sentence)) for sentence in X]\n",
    "y = [sentence + [tag2index[\"--PADDING--\"]] * (MAX_SENTENCE - len(sentence)) for sentence in y]\n",
    "print(\"X[0]:\", X[0])\n",
    "print(\"y[0]:\", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ccfd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_COUNT = len(tag2index)\n",
    "y = [ np.eye(TAG_COUNT)[sentence] for sentence in y]\n",
    "print(\"X[0]:\", X[0])\n",
    "print(\"y[0]:\", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1136f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1234)\n",
    "\n",
    "print(\"Number of sentences in the training dataset: {}\".format(len(X_train)))\n",
    "print(\"Number of sentences in the test dataset : {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9025256",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cabf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_COUNT = len(index2word)\n",
    "DENSE_EMBEDDING = 50\n",
    "LSTM_UNITS = 50\n",
    "LSTM_DROPOUT = 0.1\n",
    "DENSE_UNITS = 100\n",
    "BATCH_SIZE = 256\n",
    "MAX_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a2321",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = layers.Input(shape=(MAX_SENTENCE,))\n",
    "\n",
    "model = layers.Embedding(WORD_COUNT, DENSE_EMBEDDING, embeddings_initializer=\"uniform\", input_length=MAX_SENTENCE)(input_layer)\n",
    "\n",
    "model = layers.Bidirectional(layers.LSTM(LSTM_UNITS, recurrent_dropout=LSTM_DROPOUT, return_sequences=True))(model)\n",
    "\n",
    "model = layers.TimeDistributed(layers.Dense(DENSE_UNITS, activation=\"relu\"))(model)\n",
    "\n",
    "crf_layer = CRF(units=TAG_COUNT)\n",
    "output_layer = crf_layer(model)\n",
    "\n",
    "ner_model = Model(input_layer, output_layer)\n",
    "\n",
    "loss = losses.crf_loss\n",
    "acc_metric = metrics.crf_accuracy\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "ner_model.compile(optimizer=opt, loss=loss, metrics=[acc_metric])\n",
    "\n",
    "ner_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ner_model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949672be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
